import sys
import psycopg2
import re
import json
import tarfile
import logging
import hashlib
from pathlib import Path, PurePath
from collections import Counter
from multiprocessing import Pool

from tqdm.contrib.concurrent import process_map
from tqdm import tqdm

from extract_javascript_function import getFunctions
from classes.file import File
from classes.block import Block
from db import insert_project

FILE_EXTENSIONS = [".js", ".jsx"]
COMMENT_RE_PATTERN = r"\/\*[\s\S]*?\*\/|([^\\:]|^)\/\/.*|<!--[\s\S]*?-->$"
SEPARATORS = "; . \n [ ] ( ) ~ ! - + & * / % < > ^ | ? { } = # , \" \ : $ ' ` @".split(" ")


def get_name(raw_json):
  try:
    package_json = raw_json.decode('utf-8')
    package_json = json.loads(package_json)
  except:
    package_json = raw_json.decode('utf-8-sig')
    package_json = json.loads(package_json)

  return package_json["name"]


def tokenize_blocks(file_path: str, file_contents: str, block_line_nos, blocks: [str]) -> File:
  file_hasher = hashlib.md5()
  file_hasher.update(file_contents.encode('utf-8'))
  file_hash = file_hasher.hexdigest()

  tokenized_blocks = []
  for ([start_line, end_line], block) in zip(block_line_nos, blocks):
  # Remove tagged comments
    block = re.sub(COMMENT_RE_PATTERN, "", block)

    block_hasher = hashlib.md5()
    block_hasher.update(block.encode('utf-8'))
    block_hash = block_hasher.hexdigest()

    # Tokenize the block
    for sep in SEPARATORS:
      block = block.replace(sep, " ")
    all_tokens = [b for b in block.split(" ") if b != ""]
    block_tokens = Counter(all_tokens)

    # token_count = len(block_tokens.items())
    # unique_token_count = len(block_tokens)
    # SourcererCC formatting
    sourcerer_tokens = ["{}@@::@@{}".format(k, v) for k, v in block_tokens.items()]

    tokenized_block = Block(start_line, end_line, block_hash, sourcerer_tokens)
    tokenized_blocks.append(tokenized_block)

  return File(file_path, file_hash, tokenized_blocks)


def process_tarball(tarball, path, conn):
  tokenized_files = []
  name = "!UNKNOWN!"
  for member in tarball.getmembers():
    if not member.isfile():
      continue

    if member.name == 'package/package.json':
      raw_json = tarball.extractfile(member).read()
      try:
        name = get_name(raw_json)
      except:
        continue

    elif any(member.name.lower().endswith(ext) for ext in FILE_EXTENSIONS):
      try:
        file_contents = tarball.extractfile(member).read().decode('utf-8')
      except:
        logger.warning("Could not extract {} from {}".format(member.name, path))
        continue
      try:
        block_line_nos, blocks = getFunctions(file_contents)
      except:
        logger.warning("Could not get functions from {} in {}".format(member.name, path))
        continue

      file_path = member.name.replace("package/", "", 1) 
      tokenized_file = tokenize_blocks(file_path, file_contents, block_line_nos, blocks)
      tokenized_files.append(tokenized_file)

  insert_project(name, tokenized_files, conn)


def process_project(project_path):
  conn = psycopg2.connect(dbname="sourcerer", user="sourcerer", password="sourcerer")
  try:
    # Open tarball with transparent compression
    # (supports gzip, bz2 and lzma)
    tarball = tarfile.open(project_path, "r:*")
    process_tarball(tarball, project_path, conn)
    tarball.close()
  except tarfile.ReadError:
    logger.warning("Failed to open {}".format(project_path))
    return project_path


if __name__ == '__main__':
  if len(sys.argv) < 2:
    print("Please pass the list of project files!")
    sys.exit(1)

  log_dir = Path("logs")
  log_dir.mkdir(parents=True, exist_ok=True)
  log_path = log_dir / PurePath("output.log")
  logging.basicConfig(level=logging.DEBUG, filename=log_path)
  logger = logging.getLogger(__name__)

  project_paths_file = sys.argv[1]
  with open(project_paths_file, "r") as f:
    project_paths = [p.strip() for p in f.readlines()]

  # Process everything
  errors = []
  with Pool(processes=12) as p:
    with tqdm(total=len(project_paths)) as pbar:
      for e in p.imap_unordered(process_project, project_paths):
        pbar.update()
        errors.append(e)

  errors_path = log_dir / PurePath("errors.txt")
  with open(errors_path, "w+") as f:
    for e in errors:
      if e != None:
        f.write("{}\n".format(e))

