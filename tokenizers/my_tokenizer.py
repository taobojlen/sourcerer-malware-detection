import sys
import psycopg2
import re
import json
import tarfile
import logging
import hashlib
import tracemalloc
from pathlib import Path, PurePath
from collections import Counter
from multiprocessing import Pool
from contextlib import closing

from tqdm.contrib.concurrent import process_map
from tqdm import tqdm
from mem_top import mem_top

from extract_javascript_function import getFunctions
from classes.file import File
from classes.block import Block
from db import insert_project

tracemalloc.start()

FILE_EXTENSIONS = [".js", ".jsx"]
COMMENT_RE_PATTERN = r"\/\*[\s\S]*?\*\/|([^\\:]|^)\/\/.*|<!--[\s\S]*?-->$"
SEPARATORS = "; . \n [ ] ( ) ~ ! - + & * / % < > ^ | ? { } = # , \" \ : $ ' ` @".split(" ")


def attempt_decode(raw_data):
  try:
    data = raw_data.decode('utf-8')
  except:
    data = raw_data.decode('utf-8-sig')

  return data


def tokenize_blocks(file_path: str, file_contents: str, block_line_nos, blocks: [str]) -> File:
  file_hasher = hashlib.md5()
  file_hasher.update(file_contents.encode('utf-8'))
  file_hash = file_hasher.hexdigest()

  tokenized_blocks = []
  for ([start_line, end_line], block) in zip(block_line_nos, blocks):
  # Remove tagged comments
    block = re.sub(COMMENT_RE_PATTERN, "", block)

    block_hasher = hashlib.md5()
    block_hasher.update(block.encode('utf-8'))
    block_hash = block_hasher.hexdigest()

    # Tokenize the block
    for sep in SEPARATORS:
      block = block.replace(sep, " ")
    all_tokens = [b for b in block.split(" ") if b != ""]
    block_tokens = Counter(all_tokens)

    # token_count = len(block_tokens.items())
    # unique_token_count = len(block_tokens)
    # SourcererCC formatting
    sourcerer_tokens = ["{}@@::@@{}".format(k, v) for k, v in block_tokens.items()]

    tokenized_block = Block(start_line, end_line, block_hash, sourcerer_tokens)
    tokenized_blocks.append(tokenized_block)

  return File(file_path, file_hash, tokenized_blocks)


def process_tarball(tarball, path, conn):
  tokenized_files = []
  name = "!UNKNOWN!"
  for member in tarball.getmembers():
    if not member.isfile():
      continue

    if member.name == 'package/package.json':
      with tarball.extractfile(member) as f:
        raw_json = f.read()
        try:
          json = json.loads(attempt_decode(raw_json))
          name = json["name"]
        except:
          continue

    elif any(member.name.lower().endswith(ext) for ext in FILE_EXTENSIONS):
      with tarball.extractfile(member) as f:
        try:
          file_contents = attempt_decode(f.read())
        except:
          logger.warning("Could not extract {} from {}".format(member.name, path))
          continue
      try:
        block_line_nos, blocks = getFunctions(file_contents)
      except:
        logger.warning("Could not get functions from {} in {}".format(member.name, path))
        continue

      file_path = member.name.replace("package/", "", 1) 
      tokenized_file = tokenize_blocks(file_path, file_contents, block_line_nos, blocks)
      tokenized_files.append(tokenized_file)

  insert_project(name, tokenized_files, conn)


def process_project(project_path):
  conn = psycopg2.connect(dbname="sourcerer", user="sourcerer", password="sourcerer")
  try:
    # Open tarball with transparent compression
    # (supports gzip, bz2 and lzma)
    with closing(tarfile.open(project_path, "r:*")) as tarball:
      process_tarball(tarball, project_path, conn)
  except tarfile.ReadError:
    logger.warning("Failed to open {}".format(project_path))
    return project_path


if __name__ == '__main__':
  if len(sys.argv) < 2:
    print("Please pass the list of project files!")
    sys.exit(1)

  log_dir = Path("logs")
  log_dir.mkdir(parents=True, exist_ok=True)
  log_path = log_dir / PurePath("output.log")
  logging.basicConfig(level=logging.DEBUG, filename=log_path)
  logger = logging.getLogger(__name__)

  project_paths_file = sys.argv[1]
  with open(project_paths_file, "r") as f:
    project_paths = [p.strip() for p in f.readlines()]

  # Process everything
  errors = []
  # with Pool(processes=12) as p:
  #   with tqdm(total=len(project_paths)) as pbar:
  #     for e in p.imap_unordered(process_project, project_paths):
  #       pbar.update()
  #       errors.append(e)
  for idx, p in enumerate(tqdm(project_paths)):
    error = process_project(p)
    errors.append(error)
    if idx % 10 == 0:
      print(mem_top())

  errors_path = log_dir / PurePath("errors.txt")
  with open(errors_path, "w+") as f:
    for e in errors:
      if e != None:
        f.write("{}\n".format(e))

