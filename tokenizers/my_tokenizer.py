import sys
import re
import json
import tarfile
import logging
import hashlib
from pathlib import Path, PurePath
from collections import Counter

from tqdm.contrib.concurrent import process_map
from tqdm import tqdm

from extract_javascript_function import getFunctions
from classes.file import File
from classes.block import Block
from db import insert_project

FILE_EXTENSIONS = [".js", ".jsx"]
COMMENT_RE_PATTERN = r"\/\*[\s\S]*?\*\/|([^\\:]|^)\/\/.*|<!--[\s\S]*?-->$"
SEPARATORS = "; . [ ] ( ) ~ ! - + & * / % < > ^ | ? { } = # , \" \ : $ ' ` @".split(" ")


def tokenize_blocks(file_path: str, file_contents: str, block_line_nos, blocks: [str]) -> File:
  file_hasher = hashlib.md5()
  file_hasher.update(file_contents)
  file_hash = file_hasher.hexdigest()

  tokenized_blocks = []
  for ([start_line, end_line], block) in zip(block_line_nos, blocks):
  # Remove tagged comments
    block = re.sub(COMMENT_RE_PATTERN, "", block)

    block_hasher = hashlib.md5()
    block_hasher.update(block)
    block_hash = block_hasher.hexdigest()

    # Tokenize the block
    for sep in SEPARATORS:
      block = block.replace(sep, " ")
    block_tokens = Counter(block.split(" "))

    # token_count = len(block_tokens.items())
    # unique_token_count = len(block_tokens)
    # SourcererCC formatting
    sourcerer_tokens = ["{}@@::@@{}".format(k, v) for k, v in block_tokens]

    tokenized_block = Block(start_line, end_line, block_hash, sourcerer_tokens)
    tokenized_blocks.append(tokenized_block)

  return File(file_path, file_hash, tokenized_block)


def process_tarball(tarball, path):
  tokenized_files = []
  for member in tarball.getmembers():
    if not member.isfile():
      continue

    if member.name == 'package/package.json':
      package_json = tarball.extractfile(member).read().decode('utf-8')
      package_json = json.loads(package_json)
      name = package_json["name"]

    elif any(member.name.lower().endswith(ext) for ext in FILE_EXTENSIONS):
      file_contents = tarball.extractfile(member).read().decode('utf-8')
      try:
        block_line_nos, blocks = getFunctions(file_contents)
      except:
        logger.warning("Could not get functions from {} in {}".format(member.name, path))
        return
      
      tokenized_file = tokenize_blocks(path, file_contents, block_line_nos, blocks)
      tokenized_files.append(tokenized_file)

  insert_project(name, tokenized_files)


def process_project(project_path):
  try:
    # Open tarball with transparent compression
    # (supports gzip, bz2 and lzma)
    tarball = tarfile.open(project_path, "r:*")
    process_tarball(tarball, project_path)
    tarball.close()
  except tarfile.ReadError:
    logger.warning("Failed to open {}".format(project_path))
    return project_path


if __name__ == '__main__':
  if len(sys.argv) < 2:
    print("Please pass the list of project files!")
    sys.exit(1)

  log_dir = Path("logs")
  log_dir.mkdir(parents=True, exist_ok=True)
  log_path = log_dir / PurePath("output.log")
  logging.basicConfig(level=logging.DEBUG, filename=log_path)
  logger = logging.getLogger(__name__)

  project_paths_file = sys.argv[1]
  with open(project_paths_file, "r") as f:
    project_paths = [p.strip() for p in f.readlines()]

  # Process everything
  errors = process_map(process_project, project_paths, max_workers=1, chunksize=10)

  errors_path = log_dir / PurePath("errors.txt")
  with open(errors_path, "w+") as f:
    for e in errors:
      if e != None:
        f.write("{}\n".format(e))

