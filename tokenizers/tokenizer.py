import sys
import re
import json
import tarfile
import logging
from collections import Counter
from contextlib import closing

from tqdm.contrib.concurrent import process_map
from tqdm import tqdm
# from mem_top import mem_top

from tokenizers.extract_javascript import getFunctions
from classes.file import File
from classes.block import Block
from classes.token import Token
from util import attempt_decode, get_hash, get_conn
import db
import file_db

FILE_EXTENSIONS = [".js", ".jsx"]
COMMENT_RE_PATTERN = r"\/\*[\s\S]*?\*\/|([^\\:]|^)\/\/.*|<!--[\s\S]*?-->$"
SEPARATORS = "; . \n [ ] ( ) ~ ! - + & * / % < > ^ | ? { } = # , \" \ : $ ' ` @".split(" ")

logger = logging.getLogger(__name__)


def tokenize_blocks(block_line_nos, blocks: [str]) -> File:
  tokenized_blocks = []
  for ([start_line, end_line], block) in zip(block_line_nos, blocks):
  # Remove tagged comments
    block = re.sub(COMMENT_RE_PATTERN, "", block)
    block = re.sub(r"\s+", " ", block)
    block_hash = get_hash(block)

    # Tokenize the block
    for sep in SEPARATORS:
      block = block.replace(sep, " ")
    counted_tokens = Counter([b for b in block.split(" ") if b != ""])
    tokens = [Token(k, v) for k, v in counted_tokens.items()]

    tokenized_block = Block(start_line, end_line, block_hash, tokens)
    tokenized_blocks.append(tokenized_block)

  return tokenized_blocks


def process_file(file_contents, file_path, project_name, file_is_malicious=False) -> File:
  file_contents = re.sub(COMMENT_RE_PATTERN, "", file_contents)
  file_hash = get_hash(file_contents)

  tokenized_blocks = []
  if any(file_path.endswith(ext) for ext in FILE_EXTENSIONS) and file_contents != None:
    block_line_nos = []
    blocks = []
    try:
      block_line_nos, blocks = getFunctions(file_contents)
    except:
      logger.warning("Could not get functions from {} in {}".format(file_path, project_name))
    
    # If we didn't manage to extract any blocks, treat the file as one large block
    # This lets us tokenize scripts that don't have any functions in them
    if not blocks:
      start_line = 0
      end_line = len(file_contents.split("\n")) - 1
      block_line_nos = [(start_line, end_line)]
      blocks = [file_contents]

    tokenized_blocks = tokenize_blocks(block_line_nos, blocks)
  
  return File(file_path, file_hash, file_is_malicious, tokenized_blocks)
  

def process_tarball(tarball, path, conn):
  processed_files = []
  name = None
  version = None
  for member in tarball.getmembers():
    member_path = member.name.split("/")
    if not member.isfile() or "node_modules" in member_path:
      continue

    # Don't process files >5MB
    if member.size > 5000000:
      continue

    if member.name == 'package/package.json' or member.name == "package.json":
      with tarball.extractfile(member) as f:
        raw_json = f.read()
        try:
          parsed_json = json.loads(attempt_decode(raw_json))
          name = parsed_json["name"]
          version = parsed_json["version"]
        except Exception:
          logger.error("Failed to get name of {}".format(path))
          break
    
    else:
      with tarball.extractfile(member) as f:
        file_bytes = f.read()

      if member.name.startswith("package/"):
        file_path = member.name.replace("package/", "", 1) 
      else:
        file_path = member.name

      file_contents = None
      try:
        file_contents = attempt_decode(file_bytes)
      except:
        logger.warning("Could not extract {} from {}".format(member.name, path))
        continue

      file_obj = process_file(file_contents, file_path, path)
      processed_files.append(file_obj)

  if name != None:
    # db.insert_project(name, version, processed_files, conn)
    file_db.save_project(name, processed_files)
  else:
    logging.error("Could not find a name for {}".format(path))


def process_project(project_path):
  conn = get_conn()
  try:
    # Open tarball with transparent compression
    # (supports gzip, bz2 and lzma)
    with closing(tarfile.open(project_path, "r:*")) as tarball:
      process_tarball(tarball, project_path, conn)
  except tarfile.ReadError:
    conn.rollback()
    logger.error("Failed to open {}".format(project_path))
    return project_path
  finally:
    conn.close()


def process_malicious_project(sample_path, project_name):
  conn = get_conn()
  with open(sample_path, "r") as f:
    file_contents = f.read()

  file_obj = process_file(file_contents, "sample.js", project_name, file_is_malicious=True)

  project_id = db.get_project_id(conn, project_name)
  if project_id == None:
    project_id = db.insert_malicious_project(conn, project_name)

  db.insert_malicious_file(conn, project_id, file_obj)
  file_db.save_project(project_name, [file_obj], is_malicious=True)
